{"cells":[{"cell_type":"markdown","source":["### Loaded: Databricks Functions\n\nfor getting:\n- Avg. cluster startup time\n- Avg. nodes dropped on running clusters\n- Databricks availability\n- Clusters with unsupported runtimes\n- Number of teams active on Databricks\n- Number of users active on Databricks"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27ba5a68-269b-4da9-b727-ea5e073beb69"}}},{"cell_type":"code","source":["#Module Import\nimport concurrent.futures\nimport datetime\nimport requests\nimport pandas as pd #Only being used to make date list"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be0516d8-7abc-4ae1-9bd1-846984f63235"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#sub-functions used almost entirely for getting avg. cluster startup time + nodes lost events\n#-------------------\ndef get_events(databricks_headers\n           ,cluster_id\n           ,start_time=None\n           ,end_time=None\n           ,order=None\n           ,event_types=None\n           ,offset=None\n           ,limit=None):\n    data = {}\n    if cluster_id is not None:\n        data['cluster_id'] = cluster_id\n    if start_time is not None:\n        data['start_time'] = start_time\n    if end_time is not None:\n        data['end_time'] = end_time\n    if order is not None:\n        data['order'] = order\n    if event_types is not None:\n        data['event_types'] = event_types\n    if offset is not None:\n        data['offset'] = offset\n    if limit is not None:\n        data['limit'] = limit\n    response = requests.post(\n        'https://zalando-dummy.cloud.databricks.com/api/2.0/clusters/events',\n        headers=databricks_headers,\n        json=data\n    )\n    return response.json()\n\n#-------------------\ndef get_cluster_event(databricks_headers,cluster_id, start_time_ts, end_time_ts, event_types):\n    cluster_event_list = []\n    offset = 0\n    cluster_event = {'next_page': '', 'total_count': 99999999999}\n    while 'next_page' in cluster_event and offset < cluster_event['total_count']:\n        cluster_event = get_events(databricks_headers=databricks_headers\n                                   ,cluster_id=cluster_id\n                                   ,start_time=start_time_ts, end_time=end_time_ts\n                                   ,event_types=event_types, offset=offset\n                                   ,limit=500, order='ASC')\n        if 'events' in cluster_event:\n            cluster_event_list = cluster_event_list + cluster_event['events']\n            offset = offset + len(cluster_event['events'])\n        if 'next_page' not in cluster_event:\n            break\n    return cluster_event_list\n\n#-------------------\ndef get_node_lost_count(event_list):\n    event_types = ['NODES_LOST','RUNNING']\n    event_list = [e for e in event_list if e['type'] in event_types]\n    \n    if len(event_list) > 0:\n        #If cluster was running at all, there will be events in the list\n        return sum([1 for event in event_list if event['type'] == 'NODES_LOST'])\n    else:\n        return None\n\n#-------------------\ndef get_avg_startup_time(event_list):\n    \n    event_types = ['CREATING', 'RESTARTING', 'STARTING', 'RUNNING']\n    event_list = [e for e in event_list if e['type'] in event_types]\n    \n    start_time = None\n    \n    startup_times = []\n    \n    for event in event_list:\n        event_type = event[\"type\"]\n        \n        if event_type in ['STARTING', 'RESTARTING', 'CREATING']:\n            start_time = event['timestamp'] / 1000\n            \n        elif event_type == 'RUNNING' and start_time is not None:\n            run_time = event['timestamp'] / 1000\n            startup_times.append(datetime.datetime.fromtimestamp(run_time) - datetime.datetime.fromtimestamp(start_time))\n            start_time = None #Reseting start_time so old values will not be re-used.\n                \n    if len(startup_times) >= 1:\n        #Getting average startup time from all startups in this cluster on this day\n        total_start_time = sum([startup for startup in startup_times],datetime.timedelta())\n        avg_start_time = total_start_time / len(startup_times)\n        \n        return avg_start_time\n    \n    else:\n        return None\n\n#-------------------\ndef get_cluster_list(headers):\n    cluster_list = requests.get(\n        \"https://zalando-dummy.cloud.databricks.com/api/2.0/clusters/list\",\n        headers=headers\n    )\n    return cluster_list.json()['clusters']\n\n#-------------------\n# def get_time_pairs(first_date,last_date):\n#     dates = pd.date_range(start=first_date, end=last_date,\n#                           freq=\"1D\").to_pydatetime().tolist()\n#     #Again shifting - should definitely be utility function\n#     shift = [date + pd.Timedelta(days=1) for date in dates]\n#     return list(zip(dates, shift))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c9a5e5d-02ef-413a-bb6b-f962fc692a20"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Avg. cluster startup time per starting cluster\n#Avg. number of Node_Lost events per running cluster\n\ndef get_cluster_startup_and_nodes_lost(time_pairs, databricks_clusters, databricks_headers):\n    \n    startup_node_lost_kpis = []\n\n    for start_date, end_date in time_pairs:\n        start_time_ts = start_date.timestamp() * 1000\n        end_time_ts = end_date.timestamp() * 1000\n        event_types = ['CREATING', 'RESTARTING', 'STARTING', 'RUNNING', 'NODES_LOST']\n        events = []\n\n        #Step 1: getting all events for a particular date range:\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = []\n            for cluster in databricks_clusters:\n                futures.append(\n                    executor.submit(get_cluster_event,\n                                    databricks_headers, cluster_id=cluster['cluster_id'],\n                                    start_time_ts=start_time_ts, end_time_ts=end_time_ts,\n                                    event_types = event_types))\n            for future in concurrent.futures.as_completed(futures):\n                res = future.result()\n                events.append(res)\n\n        #Step 2: processing these events to get KPIs:\n        #Nodes Lost:\n        node_lost_count = [get_node_lost_count(cluster_events) for cluster_events in events]\n        cleaned_node_lost_count = [e for e in node_lost_count if e is not None]\n        total_node_lost_events = sum(cleaned_node_lost_count)\n        if len(cleaned_node_lost_count) > 0:\n            avg_node_lost_per_running_cluster = total_node_lost_events / len(cleaned_node_lost_count)\n        else:\n            avg_node_lost_per_running_cluster = None #If no node_lost events were reported, return None\n\n        #Startup times:\n        startup_times = [get_avg_startup_time(cluster_events) for cluster_events in events]\n        cleaned_startup_times = [e for e in startup_times if e is not None]\n        total_startup_time = sum([startup for startup in cleaned_startup_times],datetime.timedelta())\n        if len(cleaned_startup_times) > 0:\n            avg_startup_time = total_startup_time / len(cleaned_startup_times)\n        else:\n            avg_startup_time = None\n\n        #Step 3: Exporting\n        startup_node_lost_kpis.append({\n            \"date\":start_date.strftime(\"%Y-%m-%d\"),\n            #\"avg_nodes_lost\":avg_node_lost_per_running_cluster, #Note: since KPIs will likely NOT be analyzed with day granularity, this average will not be used often. Excluding to save space.\n            \"nodes_lost_total\":total_node_lost_events,\n            \"nodes_lost_cluster_count\":len(cleaned_node_lost_count),\n            #\"avg_startup_time_seconds\":avg_startup_time.total_seconds(), #Note: since KPIs will likely NOT be analyzed with day granularity, this average will not be used often. Excluding to save space.\n            \"startup_time_total_seconds\":total_startup_time.total_seconds(),\n            \"startup_time_cluster_count\":len(cleaned_startup_times)\n        })\n    return startup_node_lost_kpis"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8866b89-4300-4842-b89e-b9f328b090de"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">len cleaned events: 37\naverage cluster start time: 0:02:28.536652 between 2022-07-03 00:00:00 and 2022-07-04 00:00:00\nlen cleaned events: 80\naverage cluster start time: 0:02:33.291083 between 2022-07-04 00:00:00 and 2022-07-05 00:00:00\nlen cleaned events: 72\naverage cluster start time: 0:02:33.286626 between 2022-07-05 00:00:00 and 2022-07-06 00:00:00\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">len cleaned events: 37\naverage cluster start time: 0:02:28.536652 between 2022-07-03 00:00:00 and 2022-07-04 00:00:00\nlen cleaned events: 80\naverage cluster start time: 0:02:33.291083 between 2022-07-04 00:00:00 and 2022-07-05 00:00:00\nlen cleaned events: 72\naverage cluster start time: 0:02:33.286626 between 2022-07-05 00:00:00 and 2022-07-06 00:00:00\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Number of clusters using unsupported runtimes\n#\n#Note: Cannot be run historically - simply queries clusters as they are at runtime.\n#Therefore, if we want this data on **DAY** granularity, then this code will need to be run every day.\n#Alternatively, if we settle for *week* granularity, then this can be run once per week\n#\n\ndef get_clusters_unsupported_runtimes(cluster_list, databricks_headers):\n    \"\"\"Gets clusters that are using unsupported databricks runtime versions\"\"\"\n    \n    response = requests.get('https://zalando-e2.cloud.databricks.com/api/2.0/clusters/spark-versions', headers=databricks_headers)\n    supported_runtime_versions = response.json()['versions']\n    \n    supported_version_keys =  [version['key'] for version in supported_runtime_versions]\n    unsupported_clusters = []\n    \n    for cluster in cluster_list:\n        version = cluster['spark_version']\n        if version != 'dlt' and version not in supported_version_keys:\n            unsupported_clusters.append((\n                cluster['cluster_name'],\n                cluster['spark_version']\n            ))\n\n    return {\n        #\"date\":datetime.date.today().strftime(\"%Y-%m-%d\"),\n        #\"pct_clusters_unsupported_runtime\":len(unsupported_clusters) / len(cluster_list), #Note: since KPIs will likely NOT be analyzed with day granularity, this percent will not be used often. Excluding to save space.\n        \"dbricks_runtime_clusters_unsupported\":len(unsupported_clusters),\n        \"dbricks_runtime_clusters_total\":len(cluster_list)\n    }"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfeb9c3e-bdcf-492c-8bba-6a43fdbed718"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Number of interactive clusters vs jobs per day\n\ndef get_interactive_clusters_or_jobs(first_date_str,last_date_str):\n    spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\n    dbx_usage = spark.read.format(\"delta\").load(\"s3://zalando-datalake-binary/observability/data/databricks-usage/\")\n\n    dbx_usage.createOrReplaceTempView(\"dbx_usage_table\")\n    \n    table = spark.sql(f'''\n        WITH jobs_table AS (\n            SELECT CASE\n                    WHEN cluster_name LIKE 'API_%' THEN 1\n                    WHEN db_sku LIKE 'PREMIUM_ALL_PURPOSE_COMPUTE%' THEN 1\n                    ELSE 0\n                END AS interactive\n                ,CASE\n                    WHEN db_sku LIKE 'PREMIUM_JOBS_COMPUTE%' THEN 1\n                    ELSE 0\n                END AS job\n                ,start_date\n\n            from dbx_usage_table\n            where start_date between \"{first_date_str}\" and \"{last_date_str}\"\n        )\n        SELECT SUM(interactive) dbricks_interactive_sum\n        ,SUM(job) dbricks_job_sum\n        ,start_date date\n        FROM jobs_table\n        GROUP BY start_date\n        ORDER BY start_date\n    ''')\n    return table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53a9263d-1e4b-4c05-85d0-021b35718f4b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Get AWS Cost Per DBU\n\ndef get_aws_spend_over_dbus_ratio(first_date_str,last_date_str):\n    \n    spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\n    #Loading Delta tables from AWS S3\n    output_prefix = \"s3a://zalando-datalake-binary/observability/data\"\n    dbx_usage_daily = spark.read.format(\"delta\").load(output_prefix + \"/databricks-usage-day/\")\n    aws_usage_daily = spark.read.format(\"delta\").load(output_prefix + \"/databricks-aws-billing-day\")\n\n    #Creating views\n    team_dbx_usage_daily_df = dbx_usage_daily\n    team_aws_usage_daily_df = aws_usage_daily\n\n    team_dbx_usage_daily_df.createOrReplaceTempView(\"team_dbx_usage_daily\")\n    team_aws_usage_daily_df.createOrReplaceTempView(\"team_aws_usage_daily\")\n\n    table = spark.sql(f\"\"\"\n        WITH aws_costs AS (\n            --no need to filter by account since aws costs are only processed for nucleo-databricks right now\n            SELECT SUM(aws_cost) aws_cost_attributed_to_teams\n            ,date\n            FROM team_aws_usage_daily\n            WHERE date >= \"{first_date_str}\" AND date <= \"{last_date_str}\"\n            AND team_name != 'unknown'\n            GROUP BY date\n        ), db_costs as (\n            SELECT SUM(dbu_total) dbu_total_nucleo_databricks\n            ,date\n            FROM team_dbx_usage_daily\n            WHERE date >= \"{first_date_str}\" AND date <= \"{last_date_str}\"\n            AND workspace_name in ('zalando', 'zalando-newton')\n            GROUP BY date\n        )\n        SELECT\n               dc.dbu_total_nucleo_databricks --newton + main\n               ,ac.aws_cost_attributed_to_teams --nucleo-databricks costs that has z_team tag\n               --,ac.aws_cost_attributed_to_teams/dc.dbu_total_nucleo_databricks ratio_per_day\n               ,dc.date\n        FROM db_costs dc\n        JOIN aws_costs ac on dc.date = ac.date\n        ORDER BY 1 ASC;\n      \"\"\")\n    \n    return table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05c42e2e-76b9-4a8f-89e7-a4b9b745bf7d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Unique Active Users over given time span\n\ndef get_active_databricks_users(first_date_str,last_date_str):\n    spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\n    #Loading Delta tables from AWS S3\n    dbx_usage_daily = spark.read.format(\"delta\").load(\"s3a://zalando-datalake-binary/observability/data/databricks-audit\")\n\n    dbx_usage_daily.createOrReplaceTempView(\"audit_table\")\n\n    table = spark.sql(f'''\n        select count(distinct useridentity['email']) as databricks_user_count\n        from audit_table\n        where 1=1\n        and date between \"{first_date_str}\" and \"{last_date_str}\"\n        and (useridentity['email'] like '%@zalando.%' or useridentity['email'] like '%@tradebyte.%')\n        and useridentity['email'] not like '%+%'\n        and useridentity['email'] not like '%+_%' escape '+'\n        and useridentity['email'] not like '%-%';\n    ''')\n\n    return table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93d4eaef-713a-44d4-a8ce-bfeefd9d708a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Active Teams\n#\n#Note: This is a complex KPI to report because we want to de-duplicate our answer - requiring us to\n#ONLY query the timeframe we need (i.e., if we queried on a day level and got '12','14',and '9', it is\n#likely that there would be duplicate teams between those 3 numbers, but we wouldn't know how many - making\n#it hard to aggregate further (e.g., on a week level)\n#\n\ndef get_active_databricks_teams(first_date_str, last_date_str):\n    spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\n    #Loading Delta tables from AWS S3\n    audit_data = spark.read.format(\"delta\").load(\"s3a://zalando-datalake-binary/observability/data/databricks-audit\")\n\n    z_employees_data = spark.read.format(\"delta\").load(\"s3a://zalando-data-governance/prod/datalake/zalando_employees\")\n\n    audit_data.createOrReplaceTempView(\"audit_table\")\n    z_employees_data.createOrReplaceTempView(\"z_employees_table\")\n    \n    table = spark.sql(f'''\n        WITH db_activeusers AS\n            (SELECT distinct(useridentity_email) AS userid\n            FROM audit_table\n            WHERE 1=1\n            AND date BETWEEN \"{first_date_str}\" AND \"{last_date_str}\"\n            AND (useridentity_email like '%@zalando.%' OR useridentity_email like '%@tradebyte.%')\n            AND useridentity_email not like '%+%'\n            AND useridentity_email not like '%+_%' ESCAPE '+'\n            AND useridentity_email not like '%-%'),\n        employee_info AS\n            (SELECT email, team_team_name\n            FROM z_employees_table),\n        z_activeusers_employee AS\n            (SELECT db.userid AS user_email, ef.team_team_name AS team_name\n            FROM db_activeusers db\n            JOIN employee_info ef ON db.userid = ef.email)\n        SELECT count(DISTINCT team_name) AS active_teams\n        FROM z_activeusers_employee;\n    ''')\n    return table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39364243-e1c5-4dc0-a89c-9c301e969a58"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Availability of Databricks\n#Queried from Databricks audit logs- analysing if there is any minute without logs\n\ndef get_databricks_availability(first_date_str,last_date_str,minimum_downtime_in_minutes = 1):\n    \n    audit_data = spark.read.format(\"delta\").load(\"s3a://zalando-datalake-binary/observability/data/databricks-audit\")\n    audit_data.createOrReplaceTempView(\"audit_table\")\n    \n    table = spark.sql(f'''\n        WITH timestamp_gaps AS (\n            SELECT timestamp,\n            LAG(timestamp) OVER (ORDER BY timestamp ASC) as prev_timestamp,\n            date\n            FROM audit_table\n            WHERE date BETWEEN CAST('{first_date.date()}' AS DATE)\n            AND CAST('{last_date.date()}' AS DATE)\n            ORDER BY timestamp\n        )\n        SELECT SUM(\n            CASE\n                WHEN (timestamp - prev_timestamp)/1000/60 > {minimum_downtime_in_minutes}\n                THEN (timestamp - prev_timestamp)/1000/60\n                ELSE 0\n            END\n        ) as dbricks_downtime_in_whole_mins,\n        date\n        FROM timestamp_gaps\n        GROUP BY date\n    ''')\n\n    return table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2149f083-c121-4347-8368-9c71c84de2b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Databricks_support_functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1836758213130523}},"nbformat":4,"nbformat_minor":0}
