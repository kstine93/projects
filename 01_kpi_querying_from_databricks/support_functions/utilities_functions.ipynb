{"cells":[{"cell_type":"markdown","source":["### Loaded: Utilities Functions\n\nfor getting:\n- date_pairs from start date and end date\n- 'melting' Spark df to transform wide data to long format"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60e7fcf6-a8cd-4b57-8531-065654af7a65"}}},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql.functions import array, col, explode, lit, struct\nfrom pyspark.sql import DataFrame\nfrom typing import Iterable "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76561fbd-68da-49d9-b05a-38386b61b3c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_time_pairs(first_date,last_date):\n    '''Creates list of tuples with 2 dates each that are 1 day apart, e.g., \"[(2022-10-10, 2022-10-11)]\" '''\n    dates = pd.date_range(start=first_date, end=last_date,freq=\"1D\").to_pydatetime().tolist()\n    shift = [date + pd.Timedelta(days=1) for date in dates]\n    return list(zip(dates, shift))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf8a1dc0-897a-4bdf-adde-9809684ff18a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Taken from StackOverflow: https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe\ndef melt(\n        df: DataFrame, \n        id_vars: Iterable[str], value_vars: Iterable[str], \n        var_name: str=\"variable\", value_name: str=\"value\"):\n    \n    '''Function which takes a wide-format Spark dataframe and makes it into long-format.\n    Notes: Taken from StackOverflow: https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe'''\n    \n    # Create array<struct<variable: str, value: ...>>\n    _vars_and_vals = array(*(\n        struct(lit(c).alias(var_name), col(c).alias(value_name)) \n        for c in value_vars))\n\n    # Add to the DataFrame and explode\n    _tmp = df.withColumn(\"_vars_and_vals\", explode(_vars_and_vals))\n\n    cols = id_vars + [\n            col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n    return _tmp.select(*cols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"141e2475-f1dc-4203-8393-eedf9c38a06a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def sum_all_sparkdf_cols(df,cols_to_remove):\n    #aggregating values over all days and removing 'date' field (to match other week-level KPIs):\n    col_names = df.columns\n    for col in cols_to_remove:\n        col_names.remove(col)\n        \n    df = df.agg({col:\"sum\" for col in col_names})\n\n    #Resetting names of columns:\n    col_names = df.columns\n    col_names = [re.search(r'sum\\((.*)\\)', col).group(1) for col in col_names]\n    df = df.toDF(*col_names)\n    \n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e51f9e97-9765-4810-9026-dc1107ddfaca"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"utilities_functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":634188061730135}},"nbformat":4,"nbformat_minor":0}
