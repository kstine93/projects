{"cells":[{"cell_type":"markdown","source":["# Team Wrangler KPIs\n\n---\n\n**What:** This code calculates KPIs related to Team Wrangler.<br>\n**When:** This code should run *weekly* in order to calculate KPIs correctly.<br>\n- Note that some KPIs do not have any historical data (these are marked below), so querying the KPIs on-time (i.e., weekly) is important to get these data at all.\n- Some KPIs are collected with day-level granularity, others only with week-level granularity (this is marked below).\n\n**Where:** Results from these calculations appended to tables in the `s3://nucleo-databricks-shared-files/wrangler_kpis`\n\n>**Note:** as of the version of this notebook on Oct. 21, 2022, this notebook does not tolerate data being written to S3 outside of a regular cycle.\nThis is because there is not yet a mechanism to ensure that duplicate data is not being written. The only way to ensure this now is by ONLY writing data to S3 in regularly-scheduled intervals\nand that the interval is the same as the 'timespan_days' value below (e.g., our DB job runs this notebook once every 7 days & timespan_days is also set to 7).\n---\n\n**More details:**\nPlanning and overview of these KPIs here: https://docs.google.com/spreadsheets/d/1G0tD4O8eEj5ZC1eWxoR16jmuWxxF2IaLoPieMRiJFFo/edit#gid=596961878"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4da9f1b8-6928-417c-acdb-9b2677cbb9da"}}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b747846-f239-4010-9434-af1ba18be69c"}}},{"cell_type":"markdown","source":["####  >> Configurable parameters <<"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b178e4ea-ea11-4855-aa13-f24fb7396874"}}},{"cell_type":"code","source":["#Set how many days in the past you want the LAST DAY of your queries to look at (e.g., a value of 7 will mean that the last date of KPIs queried will be 7 days ago).\nday_delay = 2\n\n#Set how many total days you want to query KPI data for. This is subtracted from 'day_delay' and represents the FIRST DAY that your queries will look at (e.g., a value of 14 will query a 2-week range of data)\ntimespan_days = 7\n\n#Team members of 'Team Wrangler'. These are used to filter this team's responses to GitHub issues and PRs. Update as members enter / leave the team.\n#Team members last updated: Sept. 28, 2022\nteam_wrangler = [\n    'aakin',\n    'smolkuva',\n    'pmali',\n    'arukhan',\n    'hyonetani',\n    'ianiskovets',\n    'kakula'\n]\n\n#Toggle this value to exclude inclusion of KPIs which do not have historical data (and which therefore cannot be backfilled for historical dates).\n#Set this to True if you need to query data more than 1 week into the past so that you don't get current data named as historical data.\nexclude_kpis_with_no_historical_data = False\n\n#Location where result tables will be written:\noutput_bucket = \"s3://nucleo-databricks-shared-files/wrangler_kpis\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ddb0336-b70d-4e29-a9f4-0d9d2ad33fa7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from py4j.protocol import Py4JJavaError\n#Only writing to S3 if this is NOT a manual run (we don't want to write to S3 accidentally > once)\n\ntry:\n    write_results = (dbutils.widgets.get(\"triggered_by\") == \"scheduled_job\")\n    \nexcept Py4JJavaError as e:\n    write_results = False\n    print('''Error: No weekly-level data will be written to S3 (job not triggered automatically - this is a mechanism to avoid duplicate data in S3). \n    if this job was run automatically; check job parameters to ensure the correct value is being passed.''')\n    raise"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dede5dcc-e979-4a0b-991c-c0ccd9275974"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Module Import"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16386c28-6a3f-43db-a490-52b515db2f36"}}},{"cell_type":"code","source":["from datetime import datetime, timedelta\nfrom github import Github\nfrom pyspark.sql import Row\nimport logging"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"193f4b1c-36c3-457b-b862-5166400178fa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./support_functions/utilities_functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c14c5cc-e27b-422e-8bff-64df06ed6d1e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Setting date range to query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31bb1bee-b3d6-4209-9615-010746daef9b"}}},{"cell_type":"code","source":["#Setting reference_date to today\nreference_date = datetime.now()\n\n#last date should be UNTIL midnight (i.e., inclusive of that day)\nlast_date = reference_date  - timedelta(day_delay)\nlast_date = last_date.replace(hour=23,minute=59,second=59)\n#Since we are setting the last date to be inclusive of the last date, we need to decrement it by 1 so that we aren't\n#pulling in data for an extra day\nlast_date = last_date - timedelta(days = 1)\n\n#first_date should be STARTING AT midnight (i.e., inclusive of that day)\nfirst_date = reference_date - timedelta(day_delay + timespan_days)\nfirst_date = first_date.replace(hour=0,minute=0,second=0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9350535-1545-480f-a613-4c73de2c5ac6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#-------------------------------------\n#Creating derived data types for dates:\nfirst_date_str = first_date.strftime(\"%Y-%m-%d\")\nlast_date_str = last_date.strftime(\"%Y-%m-%d\")\n \n#Creating range of days between a start date and end date - stored as list\ndate_pairs = get_time_pairs(first_date,last_date)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f34ab042-a81a-4ec2-bfb9-630390831787"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Appending KPI results to list for eventual JOINing"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a650fc57-efc4-4757-bbbd-dade3438f983"}}},{"cell_type":"code","source":["weekly_kpis = {}\ndaily_kpis = {}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45ef72cd-585a-4c80-9b53-587fc50d6484"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## GitHub KPIs\nSetup:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e0350c3-d9a8-4715-b1b6-09341a1b66fe"}}},{"cell_type":"code","source":["gh_token = dbutils.secrets.get(scope = \"team-wrangler-monitoring-scope\", key = \"github_kpi_kevinstine_token\")\ngh_client = Github(base_url=\"https://github.bus.zalan.do/api/v3\", login_or_token=gh_token)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d9ebca4-9465-4812-84b6-daaaed8a4b69"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./support_functions/GitHub_support_functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c00b3bd1-0edc-4dec-98a7-5fd35a763a5b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### GitHub: Average response time to new tickets in 'Wrangler/issues' (per week)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06ce6f68-724a-43e7-b50d-48a973857a3a"}}},{"cell_type":"code","source":["gh_ticket_response_time = get_new_tickets_response_time(gh_client, team_wrangler, first_date, last_date, date_pairs)\n\n#Converting to Spark DF\ngh_ticket_response_time_df = spark.createDataFrame(Row(**x) for x in gh_ticket_response_time)\n\n#aggregating values over all days and removing 'date' field (to match other week-level KPIs):\ngh_ticket_response_time_df = sum_all_sparkdf_cols(gh_ticket_response_time_df, ['date'])\n\n\nweekly_kpis.update({\n    \"gh_ticket_response_time\":gh_ticket_response_time_df\n})\n#gh_ticket_response_time_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28d1f46b-b649-44de-b6b5-7d4528c7c812"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### GitHub: Average merge time to new pull requests in 'Wrangler/processing-platform' (per week)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c875842c-68fb-4db7-88ee-c1d56c09d2a5"}}},{"cell_type":"code","source":["gh_pr_merge_time = get_pr_merge_response_time(gh_client, team_wrangler, first_date, last_date, date_pairs)\n\n#Converting to Spark DF\ngh_pr_merge_time_df = spark.createDataFrame(Row(**x) for x in gh_pr_merge_time)\n\n#aggregating values over all days and removing 'date' field (to match other week-level KPIs):\ngh_pr_merge_time_df = sum_all_sparkdf_cols(gh_pr_merge_time_df, ['date'])\n\n\nweekly_kpis.update({\n    \"gh_pr_merge_time\":gh_pr_merge_time_df\n})\n#gh_pr_merge_time_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b212003e-8909-4991-b498-144527c5be85"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["---\n\n## Databricks KPIs\n\n---"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e0034e0-4db1-4066-8463-a39e426960d7"}}},{"cell_type":"code","source":["db_credentials = dbutils.secrets.get(scope = \"team-wrangler-monitoring-scope\", key = \"databricks_kpi_admin_token\")\ndatabricks_headers = {'Authorization': 'Bearer ' + db_credentials,'Content-Type':'application/json'}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b34ca9c-1017-4117-ad93-ad743d45ce35"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./support_functions/Databricks_support_functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ea2c233-af8c-49e6-84bf-edd1f861bb47"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Pre-loading cluster list used for multiple KPIs\ndatabricks_clusters = get_cluster_list(databricks_headers)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21ac1c92-db57-4194-9cf0-bea6c3149fa7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Databricks: AWS Costs per Databricks unit (monthly)\n>Note: Although AWS costs only come out 1x per month, this is currently being calculated on a weekly basis."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f6b1b4a-9ae1-4889-86f6-34f6f9ca812e"}}},{"cell_type":"code","source":["aws_spend_per_dbu = get_aws_spend_over_dbus_ratio(first_date_str,last_date_str)\n\n#aggregating values over all days and removing 'date' field (to match other week-level KPIs):\naws_spend_per_dbu = sum_all_sparkdf_cols(aws_spend_per_dbu, ['date'])\n\nweekly_kpis.update({\n    \"aws_spend_per_dbu\":aws_spend_per_dbu\n})\n#aws_spend_per_dbu.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f45f4001-d206-4d84-8160-df74318e53cd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Databricks: Average cluster startup time + nodes lost events (per day)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65cc54fa-6476-4709-9afb-b4a742cfb513"}}},{"cell_type":"code","source":["#Returns JSON list\ndbricks_startup_nodes = get_cluster_startup_and_nodes_lost(date_pairs, databricks_clusters, databricks_headers)\n\n#Converting to Spark DF\ndbricks_startup_nodes_df = spark.createDataFrame(Row(**x) for x in dbricks_startup_nodes)\n\ndaily_kpis.update({\n    \"dbricks_startup_nodes\":dbricks_startup_nodes_df\n})\n#dbricks_startup_nodes_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7dbdbfa-c87c-47ca-a2f8-f32a05bab37d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Databricks: Number of interactive clusters vs. jobs (per day)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"172f7358-517a-460e-8d4a-a8b4210bd5a5"}}},{"cell_type":"code","source":["#Returns Spark DF\ndbricks_interactive_clusters = get_interactive_clusters_or_jobs(first_date_str,last_date_str)\ndaily_kpis.update({\n    \"dbricks_interactive_clusters\":dbricks_interactive_clusters\n})\n#dbricks_interactive_clusters.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14834ed8-ff98-47a8-921c-c1567dc3c8f4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Databricks: Availability (per day)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6c6ee7a-aa31-4c37-a117-dcd5136e0123"}}},{"cell_type":"code","source":["#Returns Spark DF\ndbricks_databricks_availability = get_databricks_availability(first_date_str, last_date_str, minimum_downtime_in_minutes = 1)\ndaily_kpis.update({\n    \"dbricks_databricks_availability\":dbricks_databricks_availability\n})\n#dbricks_databricks_availability.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"240ec231-995d-46d0-bb98-eb0b42bcd163"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Databricks: Number of clusters with unsupported runtimes (per week)\n\n---\n\n**NOTE: No historical data available! Only gets current status. Be cautious when backfilling KPI data for previous weeks / months**\n\n---"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b902e96d-cd0a-40b0-b740-14d66a7a4654"}}},{"cell_type":"code","source":["if not exclude_kpis_with_no_historical_data:\n    #Returns JSON list\n    dbricks_unsupported_runtimes = get_clusters_unsupported_runtimes(databricks_clusters, databricks_headers)\n\n    #Converting to Spark DF\n    dbricks_unsupported_runtimes_df = spark.createDataFrame(Row(**x) for x in [dbricks_unsupported_runtimes])\n\n    weekly_kpis.update({\n        \"dbricks_unsupported_runtimes\":dbricks_unsupported_runtimes_df\n    })\n    \n#dbricks_unsupported_runtimes_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ca58fec-c97b-4e83-bca1-fc759fb92974"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Databricks: Number of teams using Databricks (per week)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"771f188c-8359-48dd-8e97-956ca44e4071"}}},{"cell_type":"markdown","source":["### TESTING:\n- [?] Returns inclusive data?\n- [?] Data is checked for accuracy?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8e11553-77b6-4e0c-85fc-230f84150a1b"}}},{"cell_type":"code","source":["#Returns Spark DF\ntry:\n    dbricks_active_teams = get_active_databricks_teams(first_date_str, last_date_str)\n    weekly_kpis.append(dbricks_active_teams)\n    #dbricks_active_teams.show()\nexcept:\n    print(\"Getting active teams on Databricks failed. Likely due to inaccessible employee data on S3.\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e78def7a-0697-436e-b0e5-4423f062d130"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Databricks: Number of unique users using Databricks (per week)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"597d4138-1fa0-42fc-b5ad-c937aa387178"}}},{"cell_type":"code","source":["#Returns Spark DF\ndbricks_active_users = get_active_databricks_users(first_date_str,last_date_str)\nweekly_kpis.update({\n    \"dbricks_active_users\":dbricks_active_users\n})\n\n#dbricks_active_users.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86533b03-4e62-41a4-8cc6-110deceaba51"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Oracle KPIs\n\nSetup:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49913068-c1a0-431e-9711-f0aa62b4d801"}}},{"cell_type":"code","source":["oracle_config = {\n    'user':dbutils.secrets.get(scope=\"team-wrangler-monitoring-scope\", key=\"oracle-user-zalando-nagios\"),\n    'password':dbutils.secrets.get(scope=\"team-wrangler-monitoring-scope\", key=\"oracle-password-zalando-nagios\"),\n    'jdbc_url':'jdbc:oracle:thin:@{}:{}/zalando.dummy.url.com',\n    'driver':'oracle.jdbc.driver.OracleDriver',\n    'host':'00.000.00.00',\n    'port':'1521'\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"480e42a5-8526-43db-94a2-5777786dcab6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./support_functions/Oracle_support_functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3892d571-9fb6-4780-9f78-5e4af7cab9a8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Oracle: Availability (per day)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee3519ff-076a-4542-a38c-f23e885baadc"}}},{"cell_type":"code","source":["oracle_availability = get_oracle_availability(oracle_config,first_date_str,last_date_str,minimum_downtime_in_minutes = 1)\ndaily_kpis.update({\n    \"oracle_availability\":oracle_availability\n})\n#oracle_availability.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e82f7de5-6e8f-40fd-b3ac-85633770a5a7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Oracle: Percentage of erroneous jobs (per day)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81dec837-f4bb-48f1-9e84-283acb1d6505"}}},{"cell_type":"code","source":["oracle_erroneous_jobs = oracle_get_erroneous_jobs(oracle_config,first_date_str,last_date_str)\ndaily_kpis.update({\n    \"oracle_erroneous_jobs\":oracle_erroneous_jobs\n})\n#oracle_erroneous_jobs.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d001883f-92c4-4e94-bac7-0d58e8c8998c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Oracle: Get % of remaining storage (per week)\n\n---\n\n**NOTE: No historical data available! Only gets current status. Be cautious when backfilling KPI data for previous weeks / months**\n\n---"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab7d4609-e5f4-49a9-ab2a-38331b0b0883"}}},{"cell_type":"code","source":["if not exclude_kpis_with_no_historical_data:\n    oracle_pct_remaining_storage = oracle_get_remaining_storage(oracle_config)\n    weekly_kpis.update({\n        \"oracle_pct_remaining_storage\":oracle_pct_remaining_storage\n    })\n    \n#oracle_pct_remaining_storage.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0f23e7b-8778-4583-a057-c499c06ce5da"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Oracle: Active Users (per week)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8be9fcb-f718-43a6-bc3a-8103e2a33d8b"}}},{"cell_type":"code","source":["oracle_active_users = oracle_get_active_users(oracle_config,first_date_str,last_date_str)\nweekly_kpis.update({\n    \"oracle_active_users\":oracle_active_users\n})\n#oracle_active_users.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a6ecc9a-bf7a-4f17-a612-e02b76def08b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["---\n\n## Combining data + uploading to central data storage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d402d23-f2de-4fe1-87f7-8193e0b6b6d7"}}},{"cell_type":"markdown","source":["#### To Do (Oct. 18)\n1. Configure both tables to fit format agreed on by Aykut + Shrini\n2. Upload test table to S3\n3. Download table and check accuracy\n4. Upload new table (append) \n5. Download table and see effects of 'append' action"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e34f15e2-3699-4444-862d-cde3ef93cf2e"}}},{"cell_type":"markdown","source":["### Day-level data:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"708325ec-7d7f-4301-875c-79984b0d1bba"}}},{"cell_type":"code","source":["#Copying in case this fails, we can retry easily\ndaily_kpis_copy = list(daily_kpis.values())\ndaily_kpi_table = daily_kpis_copy.pop()\n\n#Joining all day-level KPIs on 'date' field\nwhile(len(daily_kpis_copy) > 0):\n    new_table = daily_kpis_copy.pop()\n    daily_kpi_table = daily_kpi_table \\\n        .join(new_table, \"date\",\"full\")\n        \n\ndaily_kpi_table = daily_kpi_table.withColumnRenamed(\"date\", \"first_date\")\n\n#Duplicating 'first_date' value in 'last_date' column so that table format matches weekly data\ndaily_kpi_table = daily_kpi_table.withColumn(\"last_date\",col(\"first_date\"))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba734f55-68a0-45a6-85a0-2981e79f87d4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Converting dataframe from 'wide' format to 'long'\ncol_names = daily_kpi_table.columns\ncol_names.remove(\"first_date\")\ncol_names.remove(\"last_date\")\ndaily_kpi_table_long = melt(daily_kpi_table,id_vars=['first_date','last_date'],value_vars=col_names)\n\n#Specifying that these variables are day-level granularity\ndaily_kpi_table_long = daily_kpi_table_long.withColumn(\"granularity\",lit(\"daily\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5420ac7-b0f0-44c4-a09d-5fbbc7fb21e2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Week-level data:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3692e56-a6c1-4fa2-a50e-23d53ec0cfd1"}}},{"cell_type":"code","source":["#Aggregating day-level KPIs to be week-level:\ndaily_kpi_table_aggregated = sum_all_sparkdf_cols(daily_kpi_table, ['first_date','last_date'])\nweekly_kpis.update({\n    \"daily_kpi_table_aggregated\":daily_kpi_table_aggregated\n})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04d39ff3-e691-410d-bcff-ae4d8c6c55ec"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Copying in case this fails, we can retry easily\nweekly_kpis_copy = list(weekly_kpis.values())\nweekly_kpi_table = weekly_kpis_copy.pop()\n\nweekly_kpi_table = weekly_kpi_table.withColumn(\"last_date\",lit(last_date_str))\n\n#Joining all week-level KPIs on 'last_date' field\nwhile(len(weekly_kpis_copy) > 0):\n    new_table = weekly_kpis_copy.pop()\n    weekly_kpi_table = weekly_kpi_table \\\n        .join(\n        new_table.withColumn(\"last_date\",lit(last_date_str)),\n        \"last_date\",\n        \"full\"\n    )\n    \nweekly_kpi_table = weekly_kpi_table.withColumn(\"first_date\",lit(first_date_str))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3819cd96-82b3-4c9e-9805-b23f92725565"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Converting dataframe from 'wide' format to 'long'\ncol_names = weekly_kpi_table.columns\ncol_names.remove(\"first_date\")\ncol_names.remove(\"last_date\")\nweekly_kpi_table_long = melt(weekly_kpi_table,id_vars=['first_date','last_date'],value_vars=col_names)\n\n#Specifying that these variables are day-level granularity\nweekly_kpi_table_long = weekly_kpi_table_long.withColumn(\"granularity\",lit(\"weekly\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57ef5cc9-c00e-48b4-918c-075323f47fac"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Merging & writing tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39d9bda0-1b9b-44f4-b8c9-bccb8e0194c1"}}},{"cell_type":"code","source":["kpi_write_table = daily_kpi_table_long.union(weekly_kpi_table_long)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00818c9c-3b14-4b10-805e-567cf6ff8ce0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Only writing to S3 if this is NOT a manual run (we don't want to write to S3 accidentally > once)\n\nif write_results:\n    print(\"Writing to S3...\")\n    kpi_write_table \\\n        .write \\\n        .option(\"header\",True) \\\n        .mode(\"append\") \\\n        .parquet(output_bucket)\nelse:\n    print(\"No data written to S3 because this notebook was triggered manually. This is a mechanism to avoid duplicate data in S3.\")\n    raise"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc6fa98d-d669-40da-a581-89abf21eb9bd"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Write_KPIs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3827345010656378}},"nbformat":4,"nbformat_minor":0}
