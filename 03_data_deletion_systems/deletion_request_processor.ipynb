{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReadDeletionEventsDataLake.ipynb\n",
    "\n",
    "#### Authors:\n",
    "* The first part of this script concerning reading data from Data Lake was forked from the GitHub of 'fsanchez' on April 1, 2020 (https://github.bus.zalan.do/fsanchez/notebooks/blob/master/ReadingCustomerDeletionEventsFromDataLake.ipynb?short_path=f54a222)\n",
    "* The last part of this script concerning publishing to Nakadi was forked from the GitHub of 'eiunkar' on April 1, 2020\n",
    "(https://github.com/eiunkar/pyNakadi)\n",
    "* The rest of this script, including the API call to Qualtrics, was written by Kevin Stine\n",
    "\n",
    "#### Purpose:\n",
    "This code was created for the purpose of identifying whether any Zalando Voices members (the research panel maintained by the Voice of Customers team) have chosen to delete all of their Zalando data.\n",
    "If this is true, the code will print out a list of these Zalando Voices members so that the user can delete them from Zalando Voices.\n",
    "This code also sends a message (via Nakadi) to Zalando's compliance network to prove that we looked through these deletion requests and acted on them.\n",
    "\n",
    "##### Specific Tasks of this Code:\n",
    "1. Read in Zalando data deletion requests from the Data Lake\n",
    "2. Extract email addresses from those requests and compare them to Zalando Voices data in Qualtrics\n",
    "3. Print out matching email addresses so that the **user of this code** can manually delete those members' data from Zalando Voices\n",
    "4. Publish data deletion confirmations to Nakadi (proving that we processed these requests).\n",
    "5. Record the requests which have been processed so far and their outcomes (deleted or no customer data found) in a local file (called \"LoggedDeletionRequests.csv\") so that we don't re-process data that we've already checked.\n",
    "\n",
    "\n",
    "#### Additional Notes:\n",
    "This script, and the structure of how it is run, should eventually undergo substantial changes to make it more compliant with Zalando's architecture standards:\n",
    "1. Firstly, this script will ideally not live in DataLab. It should be re-implemented in a service which offers easier automation and easier access for coworkers (such as Amazon Web Services). **Note: This will take longer to implement and I have not prioritized this highly yet. -Kevin, July 29, 2021**\n",
    "\n",
    "2. The authorization which enables this script to interact with the Data Lake and Nakadi (by generating a ZToken) currently depend on the credentials of the user. Once this script is set up to run automatically, this authorization will not be able to depend on the user's credentials- or there will need to be a 'robot' user whose credentials can be used for authorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the User of this Script, and the User's ZToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:08:24.297805Z",
     "start_time": "2022-08-30T10:08:23.593794Z"
    }
   },
   "outputs": [],
   "source": [
    "import auth\n",
    "import getpass\n",
    "\n",
    "user  = getpass.getuser()\n",
    "#NOTE: If auth.get_valid_token() fails in DataLab, you can click the 'Z Token' button at the top of the screen\n",
    "#and it should work again.\n",
    "token = auth.get_valid_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Connection to the Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:08:24.332955Z",
     "start_time": "2022-08-30T10:08:24.315580Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyhive import presto\n",
    "\n",
    "connection = presto.connect(\n",
    "    protocol='https',\n",
    "    host='datalab.presto.zalando.net',\n",
    "    port='443',\n",
    "    username=user,\n",
    "    password=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Out Which Deletion Requests are New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:10:37.421766Z",
     "start_time": "2022-08-30T10:10:37.416755Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "#First, I need to look within my local CSV to find the last time this script was run and- by extension- the last\n",
    "#data deletion request this script processed.\n",
    "path = \"LoggedDeletionRequests.csv\"#\"/teams/team-ur/DataDeletion/LoggedDeletionRequests.csv\"\n",
    "LastDateParsed = pd.read_csv(path,usecols=['dt'],header=0).dt.tolist()\n",
    "LastDateParsed = max(LastDateParsed)\n",
    "\n",
    "#Now that I know the last time I processed deletion requests, I know that I can find all the requests from that date until\n",
    "#the current date- and thereby ensure that I've processed all requests. NOTE: This DOES mean that I could double-process\n",
    "#requests (for example, if a person requested deletion on the day I last ran this script, they could have been included in\n",
    "#that previous deletion round, as well as the current deletion round). Example, when this code was run on 28/04/2020, there were 212\n",
    "#cases which were re-processed out of ~3,500 new requests.\n",
    "Today = str(date.today())\n",
    "LastDateParsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in New Deletion Requests from Data Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, make sure you have access to the event_confidential.care_api_customer_data_deletion_requested table: https://lake.docs.zalando.net/access/access/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:10:41.876815Z",
     "start_time": "2022-08-30T10:10:39.391197Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = '''SELECT metadata_eid,dt,email_address,customer_number\n",
    "           FROM event_delta_confidential.care_api_customer_data_deletion_requested \n",
    "           WHERE dt BETWEEN '''+\"'\"+LastDateParsed+\"'\"+''' AND '''+\"'\"+Today+\"'\"\n",
    "df = pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:10:44.118712Z",
     "start_time": "2022-08-30T10:10:44.101015Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df) #Seeing how many new cases we have:\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in QuestionPro data (Zalando Voices panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:16:53.227025Z",
     "start_time": "2022-08-30T10:11:00.591715Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import json\n",
    "\n",
    "#URL to QuestionPro Directory\n",
    "url=\"https://eu.questionpro.com/a/api/questionpro.micropanel.panelDataExport?apiKey={0}\"\n",
    "\n",
    "#Customize this to where you store your personal API token. It should be in your private DataLab space (e.g. \"home/kstine/...\")\n",
    "#for security. When you save the file with your API token,the file should contain ONLY this text:\n",
    "#{\"questionpro_api_token\":\"PUT_API_TOKEN_HERE\"}\n",
    "\n",
    "#api_location = \"/nfs/aakyuez_untitled.txt\" #Aysenur's file location\n",
    "api_location = \"/nfs/Tokens/API_Token.json\" #Kevin's file location\n",
    "\n",
    "#Reading in API Token & other headers from external file\n",
    "with open (api_location) as myfile:\n",
    "    token = json.load(myfile)['questionpro_api_token']\n",
    "    \n",
    "url = url.format(token)\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "response = http.request(method='GET',url=url)\n",
    "response = json.loads(response.data.decode('utf-8'))\n",
    "\n",
    "if response['status']['id'] == 200:\n",
    "    members = response['response']['report']['data']\n",
    "    \n",
    "# with open (\"/nfs/questionPro_dummy.json\") as myfile:\n",
    "#     members = json.load(myfile)['vals']\n",
    "#print(members)\n",
    "\n",
    "#Iterating over array of objects to extract email addresses into a list\n",
    "qpro_emails = [obj['Email Address'] for obj in members]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicate which deletion requests represent Zalando Voices members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:31:23.569748Z",
     "start_time": "2022-08-30T10:31:15.836960Z"
    }
   },
   "outputs": [],
   "source": [
    "df['inQuestionPro'] = [\"customer_deleted\" if df_email in qpro_emails else \"no_customer_data_found\" for df_email in df['email_address'].tolist()]\n",
    "#If this command only returns the column headers (no rows), it means that we didn't find any customers in QuestionPro\n",
    "#who need to be deleted.\n",
    "df[df['inQuestionPro']=='customer_deleted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## IMPORTANT:\n",
    "At this point in this script, you (the user) should delete the Zalando Voices data from any email addresses above (ZV members who have requested deletion). Instructions for how to do this are in this document:\n",
    "https://docs.google.com/spreadsheets/d/1ZSQ_o2KurGlV1TzJj2FL9Z_7P90onV23xb4B5yuYxRc/edit#gid=1615454121\n",
    "\n",
    "\n",
    "\n",
    "Once you have finished deleting these members' data, run the rest of this script- which is simply notifying Zalando that we processed the requests & recording which requests we processed in a local file (for us to read in next time).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posting Confirmations of Processing to Nakadi - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:33:56.984706Z",
     "start_time": "2022-08-30T10:33:48.156964Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pynakadi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:34:02.122049Z",
     "start_time": "2022-08-30T10:34:02.108718Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyNakadi import NakadiClient, NakadiException\n",
    "import auth\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "token = auth.get_valid_token()\n",
    "#--------!!!--------\n",
    "#--------!!!--------\n",
    "#URL to which we need to be posting our event\n",
    "url = \"https://nakadi-live.aruha.zalan.do\" #Real, live URL!\n",
    "#url = \"https://nakadi-staging.aruha-test.zalan.do\" #Test URL\n",
    "#--------!!!--------\n",
    "#--------!!!--------\n",
    "\n",
    "time = datetime.utcnow().replace(tzinfo=pytz.UTC).isoformat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data in Preparation for Posting to Nakadi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:34:20.602624Z",
     "start_time": "2022-08-30T10:34:20.545748Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating series of JSONs with our data\n",
    "def jsonification(metadata_eid,inQuestionPro,time):\n",
    "    return {\n",
    "        \"metadata\": {\n",
    "        \"eid\": metadata_eid,\n",
    "        \"occurred_at\": time\n",
    "        },\n",
    "        \"deletion_request_event_id\": metadata_eid,\n",
    "        \"team_id\":\"team-ur\",\n",
    "        \"deletion_acknowledgment\": inQuestionPro\n",
    "    }\n",
    "\n",
    "toPost = pd.Series(jsonification(row.metadata_eid,row.inQuestionPro,time) for row in df.itertuples())\n",
    "\n",
    "#----------\n",
    "#Chunking requests into sets of 50 for faster pushing to Nakadi:\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "chunked_Posts = chunker(toPost,50)\n",
    "print(\"Posts chunked and ready for Nakadi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publishing Nakadi Confirmations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:34:42.686192Z",
     "start_time": "2022-08-30T10:34:30.286440Z"
    }
   },
   "outputs": [],
   "source": [
    "#Meta-data needed to publish to Nakadi\n",
    "event_type = \"infosec.customer_data_deleted\"\n",
    "client = NakadiClient(token, url)\n",
    "NakadiPosts = []\n",
    "\n",
    "for chunk in chunked_Posts:\n",
    "    try:\n",
    "        client.post_events(event_type, chunk.tolist())\n",
    "        NakadiPosts = NakadiPosts + chunk.tolist()\n",
    "    except NakadiException as ex:\n",
    "        print(f'NakadiException[{ex.code}]: {ex.msg}')\n",
    "\n",
    "print(str(len(NakadiPosts)) + \" posts sent to \" + url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double-Checking What we Sent to Nakadi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:34:47.443178Z",
     "start_time": "2022-08-30T10:34:47.438136Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Taking a look at what I've posted to Nakadi:\n",
    "#len(NakadiPosts)\n",
    "NakadiPosts[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T11:23:59.610607Z",
     "start_time": "2022-08-16T11:23:59.598773Z"
    }
   },
   "outputs": [],
   "source": [
    "#===DOUBLE-CHECKING===\n",
    "#Taking a look at what we told Nakadi that we deleted:\n",
    "deletedEids = df['metadata_eid'][df['inQuestionPro']=='customer_deleted'].tolist()\n",
    "#This confirms whether or not the EID we were supposed to delete was in the Nakadi Posts:\n",
    "deletedPosts = [post for post in NakadiPosts if post['metadata']['eid'] in deletedEids]\n",
    "deletedPosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:34:49.665121Z",
     "start_time": "2022-08-30T10:34:49.633962Z"
    }
   },
   "outputs": [],
   "source": [
    "#===DOUBLE-CHECKING===\n",
    "#Did we tell Nakadi that we deleted the right people?\n",
    "df[df['inQuestionPro']=='customer_deleted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recording our Activity to Local \"LoggedDeletionRequests.csv\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T10:34:54.334093Z",
     "start_time": "2022-08-30T10:34:52.753803Z"
    }
   },
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "import subprocess\n",
    "\n",
    "#Note: We're going to write changes to the master version of the \"LoggedDeletionRequests\" file in the 'team' folder\n",
    "Newdf = df.drop(columns='email_address')\n",
    "with open('LoggedDeletionRequests.csv','a') as csv_file:\n",
    "    csv_writer = writer(csv_file)\n",
    "    [(csv_writer.writerow(newRow[1])) for newRow in Newdf.iterrows()]\n",
    "print(str(len(df)) + \" rows written\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
